"""
ä¸»ç¨‹åºå…¥å£ - æ·±åº¦å­¦ä¹ å›å½’é¢„æµ‹ç³»ç»Ÿ
"""
import argparse
import sys
import os
from datetime import datetime

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='æ·±åº¦å­¦ä¹ å›å½’é¢„æµ‹ç³»ç»Ÿ')
    parser.add_argument('--mode', type=str, default='gui', 
                       choices=['gui', 'train', 'inference', 'analysis'],
                       help='è¿è¡Œæ¨¡å¼: gui(ç•Œé¢), train(è®­ç»ƒ), inference(æ¨ç†), analysis(åˆ†æ)')
    parser.add_argument('--models', type=str, nargs='+', default=['mlp'],
                       choices=['mlp', 'resnet', 'transformer', 'ensemble'],
                       help='é€‰æ‹©æ¨¡å‹ç±»å‹')
    parser.add_argument('--groups', type=str, nargs='+', 
                       default=['group_1', 'group_2', 'group_3', 'group_4', 'group_5'],
                       help='é€‰æ‹©æ•°æ®ç»„')
    parser.add_argument('--x_file', type=str, default=None,
                       help='è¾“å…¥æ•°æ®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--y_file', type=str, default=None,
                       help='è¾“å‡ºæ•°æ®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--epochs', type=int, default=None,
                       help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch_size', type=int, default=None,
                       help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--learning_rate', type=float, default=None,
                       help='å­¦ä¹ ç‡')
    
    args = parser.parse_args()
    
    print("ğŸ§  æ·±åº¦å­¦ä¹ å›å½’é¢„æµ‹ç³»ç»Ÿ")
    print("=" * 50)
    print(f"è¿è¡Œæ¨¡å¼: {args.mode}")
    print(f"æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 50)
    
    if args.mode == 'gui':
        run_gui()
    elif args.mode == 'train':
        run_training(args)
    elif args.mode == 'inference':
        run_inference(args)
    elif args.mode == 'analysis':
        run_analysis()
    else:
        print(f"æœªçŸ¥æ¨¡å¼: {args.mode}")
        sys.exit(1)

def run_gui():
    """è¿è¡ŒGUIç•Œé¢"""
    try:
        print("å¯åŠ¨Gradioç•Œé¢...")
        from gradio_app import main as gradio_main
        gradio_main()
    except ImportError as e:
        print(f"å¯¼å…¥Gradioæ¨¡å—å¤±è´¥: {e}")
        print("è¯·ç¡®ä¿å·²å®‰è£…æ‰€æœ‰ä¾èµ–: pip install -r requirements.txt")
    except Exception as e:
        print(f"å¯åŠ¨GUIå¤±è´¥: {e}")

def run_training(args):
    """è¿è¡Œè®­ç»ƒ"""
    try:
        print("å¼€å§‹è®­ç»ƒæ¨¡å¼...")
        
        from config import Config
        from data_processor import DataProcessor
        from trainer import Trainer
        
        # æ›´æ–°é…ç½®
        config = Config()
        if args.epochs:
            if args.epochs <= 0:
                print("é”™è¯¯: è®­ç»ƒè½®æ•°å¿…é¡»ä¸ºæ­£æ•´æ•°")
                return
            config.EPOCHS = args.epochs
        if args.batch_size:
            if args.batch_size <= 0:
                print("é”™è¯¯: æ‰¹æ¬¡å¤§å°å¿…é¡»ä¸ºæ­£æ•´æ•°")
                return
            config.BATCH_SIZE = args.batch_size
        if args.learning_rate:
            if args.learning_rate <= 0:
                print("é”™è¯¯: å­¦ä¹ ç‡å¿…é¡»ä¸ºæ­£æ•°")
                return
            config.LEARNING_RATE = args.learning_rate
        
        print(f"é…ç½®: epochs={config.EPOCHS}, batch_size={config.BATCH_SIZE}, lr={config.LEARNING_RATE}")
        
        # æ•°æ®å¤„ç†
        print("å¤„ç†æ•°æ®...")
        processor = DataProcessor(config)
        
        if args.x_file and args.y_file:
            if not os.path.exists(args.x_file):
                print(f"é”™è¯¯: è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.x_file}")
                return
            if not os.path.exists(args.y_file):
                print(f"é”™è¯¯: è¾“å‡ºæ–‡ä»¶ä¸å­˜åœ¨: {args.y_file}")
                return
            X, y = processor.load_data(args.x_file, args.y_file)
            processed_groups = processor.process_all_groups(X, y)
        else:
            processed_groups = processor.process_all_groups()
        
        # è¿‡æ»¤é€‰ä¸­çš„ç»„
        if args.groups:
            valid_groups = [g for g in args.groups if g in processed_groups]
            if not valid_groups:
                print(f"é”™è¯¯: æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ•°æ®ç»„: {args.groups}")
                return
            processed_groups = {k: v for k, v in processed_groups.items() if k in valid_groups}
        
        # éªŒè¯æ¨¡å‹ç±»å‹
        valid_models = ['mlp', 'resnet', 'transformer', 'ensemble']
        invalid_models = [m for m in args.models if m not in valid_models]
        if invalid_models:
            print(f"é”™è¯¯: æ— æ•ˆçš„æ¨¡å‹ç±»å‹: {invalid_models}")
            print(f"æ”¯æŒçš„æ¨¡å‹ç±»å‹: {valid_models}")
            return
        
        # è®­ç»ƒ
        print(f"è®­ç»ƒæ¨¡å‹: {args.models}")
        print(f"æ•°æ®ç»„: {list(processed_groups.keys())}")
        
        trainer = Trainer(config)
        results = trainer.train_all_groups(processed_groups, args.models)
        
        print("è®­ç»ƒå®Œæˆ!")
        print("ç»“æœå·²ä¿å­˜åˆ° logs/ ç›®å½•")
        
    except Exception as e:
        print(f"è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

def run_inference(args):
    """è¿è¡Œæ¨ç†"""
    try:
        print("å¼€å§‹æ¨ç†æ¨¡å¼...")
        
        from inference import load_all_models
        import numpy as np
        
        # åŠ è½½æ¨¡å‹
        print("åŠ è½½æ¨¡å‹...")
        evaluator = load_all_models()
        
        if not evaluator.inference.models:
            print("æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œè¯·å…ˆè¿è¡Œè®­ç»ƒ")
            return
        
        print(f"æˆåŠŸåŠ è½½ {len(evaluator.inference.models)} ä¸ªæ¨¡å‹")
        
        # ç¤ºä¾‹æ¨ç†
        print("æ‰§è¡Œç¤ºä¾‹æ¨ç†...")
        X_sample = np.random.randn(5, 7)  # 5ä¸ªæ ·æœ¬
        
        for group in args.groups:
            for model_type in args.models:
                model_key = f"{group}_{model_type}"
                
                if model_key in evaluator.inference.models:
                    try:
                        predictions = evaluator.inference.predict(model_key, X_sample, group)
                        print(f"{model_key} é¢„æµ‹å®Œæˆï¼Œè¾“å‡ºå½¢çŠ¶: {predictions.shape}")
                    except Exception as e:
                        print(f"{model_key} é¢„æµ‹å¤±è´¥: {e}")
        
        print("æ¨ç†å®Œæˆ!")
        
    except Exception as e:
        print(f"æ¨ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

def run_analysis():
    """è¿è¡Œåˆ†æ"""
    try:
        print("å¼€å§‹åˆ†ææ¨¡å¼...")
        
        from hyperparameter_analysis import HyperparameterAnalyzer
        
        # ç”Ÿæˆè¶…å‚æ•°åˆ†ææŠ¥å‘Š
        print("ç”Ÿæˆè¶…å‚æ•°åˆ†ææŠ¥å‘Š...")
        analyzer = HyperparameterAnalyzer()
        analyzer.save_analysis_report()
        
        # å¦‚æœæœ‰è®­ç»ƒç»“æœï¼Œç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
        if os.path.exists('logs/training_results.json'):
            print("ç”Ÿæˆè®­ç»ƒç»“æœåˆ†æ...")
            from inference import load_all_models
            
            evaluator = load_all_models()
            # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤šåˆ†æåŠŸèƒ½
        
        print("åˆ†æå®Œæˆ!")
        print("æŠ¥å‘Šå·²ä¿å­˜åˆ°å½“å‰ç›®å½•")
        
    except Exception as e:
        print(f"åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

def check_dependencies():
    """æ£€æŸ¥ä¾èµ–"""
    required_packages = [
        'torch', 'numpy', 'pandas', 'sklearn', 
        'matplotlib', 'seaborn', 'gradio', 'tqdm'
    ]
    
    missing_packages = []
    
    for package in required_packages:
        try:
            __import__(package)
        except ImportError:
            missing_packages.append(package)
    
    if missing_packages:
        print("âŒ ç¼ºå°‘ä»¥ä¸‹ä¾èµ–åŒ…:")
        for package in missing_packages:
            print(f"  - {package}")
        print("\nè¯·è¿è¡Œ: pip install -r requirements.txt")
        return False
    
    print("âœ… æ‰€æœ‰ä¾èµ–åŒ…å·²å®‰è£…")
    return True

def show_help():
    """æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯"""
    help_text = """
ğŸ§  æ·±åº¦å­¦ä¹ å›å½’é¢„æµ‹ç³»ç»Ÿ - ä½¿ç”¨æŒ‡å—

åŸºæœ¬ç”¨æ³•:
  python main.py [é€‰é¡¹]

è¿è¡Œæ¨¡å¼:
  --mode gui          å¯åŠ¨Webç•Œé¢ (é»˜è®¤)
  --mode train        å‘½ä»¤è¡Œè®­ç»ƒæ¨¡å¼
  --mode inference    æ¨ç†æ¨¡å¼
  --mode analysis     åˆ†ææ¨¡å¼

è®­ç»ƒé€‰é¡¹:
  --models mlp resnet transformer    é€‰æ‹©æ¨¡å‹ç±»å‹
  --groups group_1 group_2          é€‰æ‹©æ•°æ®ç»„
  --epochs 200                      è®­ç»ƒè½®æ•°
  --batch_size 64                   æ‰¹æ¬¡å¤§å°
  --learning_rate 0.001             å­¦ä¹ ç‡
  --x_file path/to/x_input.csv      è¾“å…¥æ•°æ®æ–‡ä»¶
  --y_file path/to/y_output.csv     è¾“å‡ºæ•°æ®æ–‡ä»¶

ä½¿ç”¨ç¤ºä¾‹:
  # å¯åŠ¨Webç•Œé¢
  python main.py

  # è®­ç»ƒMLPå’ŒResNetæ¨¡å‹
  python main.py --mode train --models mlp resnet

  # è®­ç»ƒæŒ‡å®šç»„çš„æ•°æ®
  python main.py --mode train --groups group_1 group_2 --epochs 100

  # ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®è®­ç»ƒ
  python main.py --mode train --x_file data/x.csv --y_file data/y.csv

  # è¿è¡Œæ¨ç†
  python main.py --mode inference --models mlp --groups group_1

  # ç”Ÿæˆåˆ†ææŠ¥å‘Š
  python main.py --mode analysis

æ›´å¤šä¿¡æ¯è¯·æŸ¥çœ‹ README.md
"""
    print(help_text)

if __name__ == "__main__":
    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ˜¾ç¤ºå¸®åŠ©
    if len(sys.argv) > 1 and sys.argv[1] in ['-h', '--help', 'help']:
        show_help()
        sys.exit(0)
    
    # æ£€æŸ¥ä¾èµ–
    if not check_dependencies():
        sys.exit(1)
    
    # è¿è¡Œä¸»ç¨‹åº
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nç¨‹åºè¿è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)